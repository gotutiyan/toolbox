How Good (really) are Grammatical Error Correction Systems? の論文を紹介してみます．

```
@inproceedings{rozovskaya-roth-2021-good,
    title = "How Good (really) are Grammatical Error Correction Systems?",
    author = "Rozovskaya, Alla  and
      Roth, Dan",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.231",
    pages = "2686--2698",
}
```

# 概要

文法誤り訂正（GEC, Grammatical Error Correction）の参照あり評価（事前に用意した正解文を使って行う評価）では，用意された正解文が正解の集合を網羅できない問題がある．そこで，システム出力文をできるだけ変えないように人手で修正を加えて，それを正解文とみなして評価した．英語とロシア語のデータセットで評価した結果，GECシステムは従来報告されているよりも高品質な訂正が行えることを示唆した．また，10-bestの出力文の分析からは，下位のランクほど多様な訂正が行えていることを示唆した．

# 導入

現状の参照あり評価は，GECシステムの性能を過小評価していると主張しています．GECは入力文の誤りを発見して自動で訂正するタスクですが，訂正の候補は山ほどあります．現在使われているM^2 ScorerやGLEU，ERRANTのような評価指標はいずれも参照あり評価で，事前に用意された正解文をもとに評価しています．正解文は人手でアノテーションしており，データセットによっては一つの入力文に対して正解文が複数付与されることがありますが，前述したように正解の候補は大量にあるため全てを網羅できません．よって，現状の参照あり評価は，GECシステムが持つ本当の性能を評価できていないとしています．また，こうなる主な原因は，正解文がシステム出力とは独立に作成されているから，とも主張しています．

# 手法

前述した問題は，システム出力から正解文を作ることで解決します．こうして作られた正解文もまた入力文に対応する正解の集合に属するため，本来は評価されるべき部分が評価されると言えます．論文中では，このように作成した正解文をClosest Golds (CGs)と呼んでいます．これと対応して，元からアノテーションされている正解文はReference Gold (RG) と呼んでいます．

CGsは，入力文とシステム出力文を見ながら，入力文の意味を損なわず，かつ，システム出力文との編集距離ができるだけ小さくなるように作ります．形式的には，入力文を[tex:S]，確率が高い順に10文出力させたシステム出力文を順に[tex:H_1, H_2\dots H_{10}]とすると，[tex:(S,H_i)]の組みを見ながら，対応する[tex:CG_i]を作ることになります．結局，全体的としては，[tex:S,H_i,RG,CG_i]のセットが得られる感じです．

# 実験
実験は英語のデータセットとロシア語のデータセットを用いて行っています．英語データセットはCoNLL-2014とBEA test，ロシア語のデータセットはRULEC-GECとlang-8コーパスのロシア語の文です．GECモデルは，英語にはBERTベースのモデル，ロシア語にはTransformerベースのSoTAモデルを使います．

[tex:CG_i]は，[tex:i=1,2,5,10]について作ります．つまり，ある入力文に対してGECシステムが1,2,5,10番目だと推定した4つの出力文について，それぞれを人手で編集して[tex:CG_i]を作ります．入力文は，各データセットから100文ずつランダムに取ります．[tex:CG]のアノテーションは，英語ではネイティブと非ネイティブの2人，ロシア語は1人のネイティブが行います．3人とも修士号を有しており，過去にアノテーションの経験があるそうです．

# 結果
モデルの出力は[tex:RG]に対して最適化されていることを主張しています．まず，[tex:RG]に（赤色）に注目すると[tex:H_1]から[tex:H_10]にかけてスコアが減少していて．かつ，[tex:H_1]と[tex:H_2]の差が特に大きく開いています．一方で，[tex:CG]（青色）に注目すると，[tex:RG]ほどその傾向は表れていません．つまり，モデルの出力文でランクが低いものでも，（数値上は）ランクが高いものと遜色ない訂正が行えていると言えます．

また，訂正が行われる数に注目すると，ランクが低い出力文ほど多くの訂正を行っており，かつ，RGに対するgold editとCGに対するgold editの数の差が大きくなることを示しています（Table2）．ここで，gold editとはモデルが訂正した誤りで，かつ，それがRGやCGにも存在するようなものを指します．つまり，ランクが高い出力文ほどRG，つまり与えられた訂正に近いような訂正にフィットしていることが言えます．（まあtrainもtestも同じアノテータがつけているので，trainにフィットさせたモデルが自信を持って出力する文はtestの正解（RG）にもフィットするだろうという気はします．）

次に，編集率（Edit Rate）に注目した結果も分析しています．ここでは主にpost-editと呼ばれる，正解文であるCGとシステム出力文の間の編集距離として定義された値を計算しています．結果として，英語のデータセットではランクによるpost-editに大きな違いはなく，ロシア語のデータセットでは[tex:H_5]や[tex:H_10]はわずかにpost-editが大きくなっているが，[tex:H_1]と[tex:H_2]の差は特にないという結果になっています．ロシア語のデータセットでは[tex:H_5]や[tex:H_10]でpost-editの値が大きくなりますが，大幅に劣化しているわけではないので，やはり下位のランクの出力文も高品質ですねという結論になっています．

最後に，英語のデータセットで出力文のランクにおける訂正の傾向について調べると，低いランクの出力文ほどLexicalな訂正を行っていることが示唆されています．Table5では上のブロックがスペル誤り/文法誤りでグルーピングした誤りタイプを，下のブロックがLexicalな誤りでグルーピングした誤りタイプを示しています．Table5最下部のパーセンテージを見ると，[tex:H_1]よりも[tex:H_10]のほうがLexicalな誤りを多く含んでいることがわかります．つまり，実は低いランクの出力文のほうが多様な誤りを訂正できているということです．

